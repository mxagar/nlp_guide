{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "500ed7c1-4808-430c-b281-c9e045f0316a",
   "metadata": {},
   "source": [
    "# Spacy Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2770051-bfc9-491a-84b4-21f411a2445b",
   "metadata": {},
   "source": [
    "The two main NLP libraries we are going to use are **Spacy** and **NLTK**.\n",
    "\n",
    "Main differences of the two libraries:\n",
    "- NLTK was released in 2001 and it has several algorithms and models implemented.\n",
    "- Spacy was released in 2015 and it has the best and fastest methods only; it can be more than 100x faster than NLTK.\n",
    "\n",
    "Spacy can have a tricky installation: look at [Spacy Installation](https://spacy.io/usage). Take into account that we need to download the dictionaries, too. I installed everything as follows:\n",
    "\n",
    "```bash\n",
    "conda install keras nltk\n",
    "conda install -c conda-forge spacy\n",
    "# Download dictionaries/models\n",
    "python -m spacy download en # spacy.load('en_core_news_sm')\n",
    "python -m spacy download es # spacy.load('es_core_news_sm')\n",
    "python -m spacy download de # spacy.load('de_core_news_sm')\n",
    "```\n",
    "\n",
    "Both libraries are used to perform **Natural Language Processing**, which consists in parsing and structuring the raw text so that it can be handled by the computer.\n",
    "\n",
    "Overview of contents:\n",
    "\n",
    "*Diclaimer: I made this notebook while following the Udemy course [NLP - Natural Language Processing with Python](https://www.udemy.com/course/nlp-natural-language-processing-with-python/) by Jos√© Marcial Portilla. The original course notebooks and materials were provided with a download link, I haven't found a repository to fork from.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57055287-8dd7-4a2f-8eac-fca5ac93cd18",
   "metadata": {},
   "source": [
    "## 1. Model, Doc, Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e53cf0c2-131f-432d-b5cf-24a8a072e1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09f079e1-c652-43cd-8fe5-4d3950cd580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load our English _model_\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "300ccf58-5311-4bda-b715-9a4ec0d17acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a _Doc_ object:\n",
    "# the nlp model processes the text \n",
    "# and saves it structured in the Doc object\n",
    "# u: Unicode string (any symbol, from any language)\n",
    "doc = nlp(u'Tesla is looking at buying U.S. startup for $6 million')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fff95149-37fe-4a33-9f48-0caaa3297e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla PROPN nsubj\n",
      "is AUX aux\n",
      "looking VERB ROOT\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.S. PROPN dobj\n",
      "startup VERB dep\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "6 NUM compound\n",
      "million NUM pobj\n"
     ]
    }
   ],
   "source": [
    "# Print each token separately\n",
    "# Tokens are word representations, unique elements\n",
    "# Note that spacy does a lot of identification work already\n",
    "# $ is a symbol, U.S. is handled as a word, etc.\n",
    "for token in doc:\n",
    "    # token.text: raw text\n",
    "    # token.pos_: part of speech: proper noun, verb, ...\n",
    "    # token.dep_: syntactic dependency\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ba3eb96-00e0-4c23-82f4-789aa58924f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7fbd3c548ec0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7fbd3c548bb0>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7fbd18bc3050>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7fbd18b55dc0>),\n",
       " ('lemmatizer',\n",
       "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7fbd18b6dd20>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7fbd18b31a50>)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The Doc object contains the processed text\n",
    "# To see how it is processed, we can show the pipeline used\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c262c2f1-554d-40a1-bbd5-f39554690780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can get the basic names of the steps in the pipeline\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30d08ec-dfb5-465b-8cff-4903bfe5dfb8",
   "metadata": {},
   "source": [
    "For a more detailed explaination of pipelines and their steps, see: [Spacy Pipelines](https://spacy.io/usage/spacy-101#pipelines)\n",
    "\n",
    "![Spacy Pipeline](../pics/spacy_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53e3774-88da-43f2-be88-67b02f20a84b",
   "metadata": {},
   "source": [
    "## 2. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b6dab-6c06-4b3d-b5a7-a4b391e3c446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
