{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f30376cf-4a99-49e7-a725-1f58bc0f9766",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be31af7-bdfe-45b3-a848-31b3dc3e344d",
   "metadata": {},
   "source": [
    "Once we have broken down the text into separate tokens, the next step in NLP is **stemming**, which consists in extracting the base form of each token. A word can have many variations; we call **stem** to the original or root form without variations. For example:\n",
    "\n",
    "`boat -> boats, boating, boater`\n",
    "\n",
    "Removing final parts to get the stem is not straightforward, since it is full of exceptions; therefore, Spacy does not have a stemmer, but instead, it performs directly **lemmatization**. However, since stemming is a known process in NLP, we're going to try it with [NLTK](https://www.nltk.org/).\n",
    "\n",
    "Two important stemming algorithms are were ddeveloped by Martin Porter (in 1980):\n",
    "- Porter Stemmer\n",
    "- Snowball Stemmer (developed later, based on the first; it improves the speed)\n",
    "\n",
    "The algorithms use five phases of word reduction, each with its own set of mapping rules.  For instance, in the first phase, easy suffixes are simplified; from all rules in a phase, the one which achieves the largest reduction is applied:\n",
    "\n",
    "`SSES -> SS:  caresses -> caress`\n",
    "`IES -> I:    ponies -> poni`\n",
    "\n",
    "In later phases, more complex mappings are applied, which take more variables into account:\n",
    "\n",
    "`ATIONAL -> ATE:   relational -> relate; national -> national`\n",
    "\n",
    "Note that many exceptions arise. Additionally, each language requires its own stemmer.\n",
    "\n",
    "Overview of contents:\n",
    "1. Porter Stemmer\n",
    "2. Snowball Stemmer\n",
    "\n",
    "*Diclaimer: I made this notebook while following the Udemy course [NLP - Natural Language Processing with Python](https://www.udemy.com/course/nlp-natural-language-processing-with-python/) by JosÃ© Marcial Portilla. The original course notebooks and materials were provided with a download link, I haven't found a repository to fork from.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba3588f-ca01-4340-8fc9-9d61e8e78d4c",
   "metadata": {},
   "source": [
    "## 1. Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b86f0f6f-b0d9-4b11-b092-e60d0df0ec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1902c13-4fb3-419e-a922-f632e8a1dbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c44b02a-a98a-4c2c-8285-ef7222392f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fd1266a-d1a7-49f8-a494-d2e8b35dfc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['run','runner','running','ran','runs','easily','fairly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b40ebe0e-5dbc-47d1-bba0-db18d70973e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fairli\n"
     ]
    }
   ],
   "source": [
    "# Note that:\n",
    "# - all variations of run are converted to run/ran\n",
    "# - we get easili and fairli, particular words\n",
    "for word in words:\n",
    "    print(word+' --> '+p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f59b600-feaa-452c-9679-30df0e84c0ba",
   "metadata": {},
   "source": [
    "## 2. Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "147bab11-c34c-4344-bf61-8194c9558b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efd7ba8e-85c2-46d3-819a-638bf8d13561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Snowball Stemmer requires that you pass a language parameter\n",
    "s_stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd2c1bb1-7a51-4857-a097-020e117f536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['run','runner','running','ran','runs','easily','fairly']\n",
    "# words = ['generous','generation','generously','generate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d5a10d8-9379-471a-bad8-8ca96daa6ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fair\n"
     ]
    }
   ],
   "source": [
    "# Now, fairly is converted to fair\n",
    "for word in words:\n",
    "    print(word+' --> '+s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c31b57-85e6-429a-8b26-5c69efbc13a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aeaa17-5e53-405b-918f-7c0897ac98d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
