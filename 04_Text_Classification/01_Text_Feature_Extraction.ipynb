{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "529a8d83-1f65-4150-aeb2-75c372cef609",
   "metadata": {},
   "source": [
    "# Text Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a04c7f5-e311-4e44-a5d2-d65348f24c76",
   "metadata": {},
   "source": [
    "This document explains the basics of text vectorization using TFIDF. We are going to work on the [SMS Spam Dataset @ UCI](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) as a starter to check the basic ML pipeline with Scikit-Learn. The dataset labels an SMS text as `spam` or `ham`.\n",
    "\n",
    "In order to improve the SMS classification or any similar NLP problem, we need to vectorize the text, i.e., convert words/tokens/strings into numerical values. One approach consists in creating a vector which contains an element for every unique/possible word in our texts: that generic vector to be filled in is a **vocabulary**. Each SMS/text is transformed into such a vector, and, in that form, it is considered to be a **bag of words**. With those bags of words, we can\n",
    "\n",
    "- For each SMS or text document, count the number of times a word/term occurs; that way, we get the **Document Term Matrix**.\n",
    "- A better alternative consists in weighting the term frequency in each text with the inverse frequency of appearance in all the texts: **TF-IDF, Term Frequency, Inverse Document Frequency**. This approach takes into account the importance of a word in the whole corpus, i.e., the complete text dataset. As such, stop words get a lower weight, thus less importance.\n",
    "\n",
    "The TFIDF formulas are as follows (although everything is automatically computed):\n",
    "\n",
    "- $\\textrm{tfidf}(t,d,D) = \\textrm{tf}(t,d) \\cdot \\textrm{idf}(t,D)$\n",
    "\n",
    "- $\\textrm{tf}(t,d) = \\textrm{count}(t \\in d)$: `count (term t in document d)`\n",
    "\n",
    "- $\\textrm{idf}(t,D) = \\log \\frac{N}{| \\{d \\in D \\, : t \\in d \\} |}$: `log (total documents N / documents which contain term t)`\n",
    "\n",
    "Note that the concept of bags of words and the vocabulary can be improved:\n",
    "- Instead of words, we can used tokens that have been stemmed.\n",
    "- In addition to counting the stemmed tokens, we can use additional information for each of them: morphological information (`pos_`) and syntactic information (`dep_`). Thus we end up having highly dimensional and sparse hypermatrices (tensors).\n",
    "\n",
    "Overview of contents:\n",
    "\n",
    "1. Manual Creation of Bags-of-Words\n",
    "2. Text Feature Extraction with Scikit-Learn\n",
    "   - 2.1 Load Dataset and Explore It\n",
    "   - 2.2 Train/Test Split\n",
    "   - 2.3 Vectorization\n",
    "     - 2.3.1 Document Term Matrix: `CountVectorizer`\n",
    "     - 2.3.2 Term Frequency Inverse Document Frequency: `TfidVectorizer`\n",
    "   - 2.4 Model: Definition and Training\n",
    "   - 2.5 Build a Pipeline\n",
    "   - 2.6 Evaluate the Pipeline/Model\n",
    "   - 2.7 Inference\n",
    "\n",
    "*Diclaimer: I made this notebook while following the Udemy course [NLP - Natural Language Processing with Python](https://www.udemy.com/course/nlp-natural-language-processing-with-python/) by Jos√© Marcial Portilla. The original course notebooks and materials were provided with a download link, I haven't found a repository to fork from.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cff9246-989c-473f-a2d4-6b950542ecaf",
   "metadata": {},
   "source": [
    "## 1. Manual Creation of Bags-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eec19eb-7bd6-4270-939f-538d2b889826",
   "metadata": {},
   "source": [
    "This is a vanilla manual creation of vocabularies and bags or words. Not done in practice, since we use scikit-learn functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eb13958-c69c-45ce-b44e-b65ab37608d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"This is a story about cats\n",
    "        our feline pets\n",
    "        Cats are furry animals\n",
    "        \"\"\"\n",
    "text2 = \"\"\"This story is about surfing\n",
    "        Catching waves is fun\n",
    "        Surfing is a popular water sport\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e3a7adf-3e54-4dba-beee-6328ad659c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [text1, text2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "946c04fb-70c3-4bb4-9a83-45224d156a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary (dictionary)\n",
    "vocab = {}\n",
    "i = 1\n",
    "for text in texts:\n",
    "    x = text.lower().split()\n",
    "    for word in x:\n",
    "        if word in vocab:\n",
    "            continue\n",
    "        else:\n",
    "            vocab[word]=i\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b9a3d82-6e99-4fad-a4fc-94c5d342737e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 1, 'is': 2, 'a': 3, 'story': 4, 'about': 5, 'cats': 6, 'our': 7, 'feline': 8, 'pets': 9, 'are': 10, 'furry': 11, 'animals': 12, 'surfing': 13, 'catching': 14, 'waves': 15, 'fun': 16, 'popular': 17, 'water': 18, 'sport': 19}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67324291-2e61-471e-ae5e-074a30d45b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "one = ['text1']+[0]*len(vocab)\n",
    "two = ['text2']+[0]*len(vocab)\n",
    "bow = [one, two]\n",
    "i = 0\n",
    "for text in texts:\n",
    "    x = text.lower().split()\n",
    "    for word in x:\n",
    "        bow[i][vocab[word]] += 1\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9abcf54-eec3-4acb-bf0f-4040454fcbba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text1', 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "272a0346-7daf-4d67-a9a4-dc34cecf558c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text2', 1, 3, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de683c9f-48c0-40ac-9dea-37c3d8341a13",
   "metadata": {},
   "source": [
    "## 2. Text Feature Extraction with Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60015a6f-ecbf-48d3-b526-64660027df6d",
   "metadata": {},
   "source": [
    "### 2.1 Load Dataset and Explore It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7eae8ce-9257-4227-be5a-0b774a2377a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8b7406b-1b6b-412b-bfb1-eca4b49bccb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "      <th>punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length  punct\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111      9\n",
       "1   ham                      Ok lar... Joking wif u oni...      29      6\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155      6\n",
       "3   ham  U dun say so early hor... U c already then say...      49      6\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61      2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/smsspamcollection.tsv', sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cdf7119-a181-40c0-9334-1d27e0cd4124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label      0\n",
       "message    0\n",
       "length     0\n",
       "punct      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing values: None\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7517be08-8edd-49ce-b496-ef08c6060542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check target: balanced? No\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bb7c74-7994-4f32-a8da-418928325fc6",
   "metadata": {},
   "source": [
    "### 2.2 Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64eb3604-b1d9-4cbe-8946-87b50ec5edfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['message']  # this time we want to look at the text\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6397c540-9c57-428d-a364-90b0edac20ac",
   "metadata": {},
   "source": [
    "### 2.3 Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a3e628-fd1a-415b-af60-4b9b048b08e4",
   "metadata": {},
   "source": [
    "#### 2.3.1 Document Term Matrix: `CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "392d0cfc-a19c-4f6c-b766-fffed587c88e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3733, 7082)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Class that creates the Document Term Matrix\n",
    "count_vect = CountVectorizer()\n",
    "# Fit the class-object to the training split and transform the split\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "# Vocabulary size: columns (number of features)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21be524e-6dbc-4ca4-ad5e-60074fefe470",
   "metadata": {},
   "source": [
    "#### 2.3.2 Term Frequency Inverse Document Frequency: `TfidVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90de7856-fe5e-4edb-89a9-fc76e9e2d722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3733, 7082)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Class that created the Document Term Matrix to be filled in with TFIDF values\n",
    "vectorizer = TfidfVectorizer()\n",
    "# Fit training split and transform it\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train) # remember to use the original X_train set\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed5bf90-87aa-4c9f-b903-be433bafccec",
   "metadata": {},
   "source": [
    "### 2.4 Model: Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "428f6b5c-7a03-464f-abe2-e2df118576eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC() # classifier: clf\n",
    "clf.fit(X_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ec1071-53e1-4e56-b047-56298c7ac855",
   "metadata": {},
   "source": [
    "### 2.5 Build a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2384003a-15f2-4df6-82e3-4b8c66ff8aab",
   "metadata": {},
   "source": [
    "Since in NLP all texts need to be pre-processed and vectorized, it is very common to create `Pipelines` in which we define all transformations required. Actually, since we need to deploy our application, every ML project should be packed into similar `Pipeline` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b2dbdfc-3681-4586-a3f8-3fae8424b982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', LinearSVC())])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.svm import LinearSVC\n",
    "\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('clf', LinearSVC()),\n",
    "])\n",
    "\n",
    "# Now, we can pass the raw dataset\n",
    "text_clf.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eff36f4-9cae-47d0-9b88-815e64a8e670",
   "metadata": {},
   "source": [
    "### 2.6 Evaluate the Pipeline/Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "116a4335-ce68-437a-8dc7-2abcbc45107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form a prediction set\n",
    "predictions = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a0c27ab-1d14-48c5-a1f9-4ab91d15f78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1586    7]\n",
      " [  12  234]]\n"
     ]
    }
   ],
   "source": [
    "# Report the confusion matrix\n",
    "# It performs much better than before!\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "017af475-f930-47d4-bf30-de79c3432bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      1.00      0.99      1593\n",
      "        spam       0.97      0.95      0.96       246\n",
      "\n",
      "    accuracy                           0.99      1839\n",
      "   macro avg       0.98      0.97      0.98      1839\n",
      "weighted avg       0.99      0.99      0.99      1839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a classification report\n",
    "print(metrics.classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eaa98255-b220-41fc-b2b8-590decad9b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.989668297988037\n"
     ]
    }
   ],
   "source": [
    "# Print the overall accuracy\n",
    "print(metrics.accuracy_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85ead19-eb77-4026-bc1e-f2c4b0eaead5",
   "metadata": {},
   "source": [
    "### 2.7 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ec23afcd-95d7-4859-acf8-48c3aac59351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham'], dtype=object)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HAM\n",
    "text_clf.predict([\"Hi, how are you doing?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "49aa272c-4bad-419a-9d4a-ed1d495db5e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['spam'], dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SPAM\n",
    "text_clf.predict([\"Congratulations! You've been selected as a winner. Send a message to 1-800-123-2345.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db3ad74-a774-4cde-b78c-34597b1bc6bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
