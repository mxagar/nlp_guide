{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "756f6bf6-4b35-4352-ab1c-01ef0c9d7edd",
   "metadata": {},
   "source": [
    "# Text Classification Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a257dc68-fb75-4a47-b2e4-fe8c48b63f95",
   "metadata": {},
   "source": [
    "This notebook shows how to perfom text classification by applying bags-of-words and TFIDF. The [Movie Review Dataset from Cornell](https://www.cs.cornell.edu/people/pabo/movie-review-data/) is used. Nothing new is shown here, just a more complex example than in previous notebook.\n",
    "\n",
    "Overview of contents:\n",
    "\n",
    "1. Load and Explore Dataset\n",
    "2. Train/Test Split\n",
    "3. Build Pipelines: Naive Bayes & Support Vector Machines with `TfidfVectorizer(stop_words)`\n",
    "4. Evaluate Pipelines\n",
    "    - 4.1 Naive Bayes\n",
    "    - 4.2 Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e6f39d-1471-437f-bb0a-6a244c188922",
   "metadata": {},
   "source": [
    "*Diclaimer: I made this notebook while following the Udemy course [NLP - Natural Language Processing with Python](https://www.udemy.com/course/nlp-natural-language-processing-with-python/) by Jos√© Marcial Portilla. The original course notebooks and materials were provided with a download link, I haven't found a repository to fork from.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc721278-e142-4bce-a0d4-c59d3be0e613",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a59d8ed-2ff6-493c-a5aa-779b2e8e3c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "71a94089-02b0-4de5-8770-b08e87a120bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>how do films like mouse hunt get into theatres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>some talented actresses are blessed with a dem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>this has been an extraordinary year for austra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>according to hollywood movies made in last few...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>my first press screening of 1998 and already i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                             review\n",
       "0   neg  how do films like mouse hunt get into theatres...\n",
       "1   neg  some talented actresses are blessed with a dem...\n",
       "2   pos  this has been an extraordinary year for austra...\n",
       "3   pos  according to hollywood movies made in last few...\n",
       "4   neg  my first press screening of 1998 and already i..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/moviereviews.tsv', sep='\\t') # TAB separator\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b7b2c83-5836-4532-a3d7-ed9949f4dd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n",
      "this has been an extraordinary year for australian films . \n",
      " \" shine \" has just scooped the pool at the australian film institute awards , picking up best film , best actor , best director etc . to that we can add the gritty \" life \" ( the anguish , courage and friendship of a group of male prisoners in the hiv-positive section of a jail ) and \" love and other catastrophes \" ( a low budget gem about straight and gay love on and near a university campus ) . \n",
      "i can't recall a year in which such a rich and varied celluloid library was unleashed from australia . \n",
      " \" shine \" was one bookend . \n",
      "stand by for the other one : \" dead heart \" . \n",
      ">from the opening credits the theme of division is established . \n",
      "the cast credits have clear and distinct lines separating their first and last names . \n",
      "bryan | brown . \n",
      "in a desert settlement , hundreds of kilometres from the nearest town , there is an uneasy calm between the local aboriginals and the handful of white settlers who live nearby . \n",
      "the local police officer has the task of enforcing \" white man's justice \" to the aboriginals . \n",
      "these are people with a proud 40 , 000 year heritage behind them . \n",
      "naturally , this includes their own system of justice ; key to which is \" payback \" . \n",
      "an eye for an eye . \n",
      "revenge . \n",
      "usually extracted by the spearing through of the recipient's thigh . \n",
      "brown , as the officer , manages quite well to keep the balance . \n",
      "he admits that he has to 'bend the rules' a bit , including actively encouraging at least one brutal \" payback \" . \n",
      " ( be warned that this scene , near the start , is not for the squeamish ) . \n",
      "the local priest - an aboriginal , but in the \" white fellas \" church - has a foot on either side of the line . \n",
      "he is , figuratively and literally , in both camps . \n",
      "ernie dingo brings a great deal of understanding to this role as the man in the middle . \n",
      "he is part churchman and part politician . \n",
      "however the tension , like the heat , flies and dust , is always there . \n",
      "whilst her husband - the local teacher - is in church , white lady kate ( milliken ) and her aborginal friend tony , ( pedersen ) have gone off into the hills . \n",
      "he takes her to a sacred site , even today strictly men-only . \n",
      "she appears to not know this . \n",
      "tony tells her that this is a special place , an initiation place . \n",
      "he then makes love to her , surrounded by ancient rock art . \n",
      "the community finds out about this sacrilegious act and it's payback time . \n",
      "the fuse is lit and the brittle inter-racial peace is shattered . \n",
      "everyone is affected in the fall out . \n",
      "to say more is to give away the details of this finely crafted film . \n",
      "suffice to say it's a rewarding experience . \n",
      "bryan brown , acting and co-producing , is the pivotal character . \n",
      "his officer is real , human and therefore flawed . \n",
      "brown comments that he expects audiences to feel warmth towards the man , then suddenly feel angry about him . \n",
      "it wasn't long ago that i visited central australia - ayers rock ( uluru ) and alice springs - for the first time . \n",
      "the wide-screen cinematography shows the dead heart of australia in a way that captures it's vicious beauty , but never deteriorates into a moving slide show , in which the gorgeous background dominates those pesky actors in the foreground . \n",
      "the cultural clash has provided the thesis for many a film ; from the western to the birdcage . \n",
      "at least three excellent australian films have covered the aboriginal people and the line between them and we anglo-saxon 'invaders' : \" jedda \" , \" the chant of jimmie blacksmith \" and \" the last wave \" . \n",
      "in a year when the race 'debate' has reared up in australia , it is nourishing to see such an intelligent , non-judgemental film as \" dead heart \" . \n",
      "the aboriginal priest best sums this up . \n",
      "he is asked to say if he is a \" black fella or white fella \" . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check some reviews\n",
    "i = 2\n",
    "print(df['label'][i])\n",
    "print(df['review'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "589f4b96-7acc-43cd-9c93-f4125831f2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total numbe of reviews\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f117d6c7-ffe3-449d-b8a1-efa0b5e34b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label      0\n",
       "review    35\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for the existence of NaN values in a cell\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f71de4e-80da-4c7e-826f-34866655c965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1965"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to remove NULL items\n",
    "df.dropna(inplace=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "250aa5ca-6d50-4039-91df-6bc02f74e3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes empty reviews are filled with spaces\n",
    "# We need to manually check them in a for-loop\n",
    "blanks = []  # start with an empty list\n",
    "for i,lb,rv in df.itertuples():  # iterate over the DataFrame\n",
    "    if type(rv)==str:            # avoid NaN values\n",
    "        if rv.isspace():         # test 'review' for whitespace\n",
    "            blanks.append(i)     # add matching index numbers to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6bfa5784-569e-4f3a-8db8-80f375a4b2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 blanks:  [57, 71, 147, 151, 283, 307, 313, 323, 343, 351, 427, 501, 633, 675, 815, 851, 977, 1079, 1299, 1455, 1493, 1525, 1531, 1763, 1851, 1905, 1993]\n"
     ]
    }
   ],
   "source": [
    "print(len(blanks), 'blanks: ', blanks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e8e388f-5fd4-4357-95e7-8cadda4f0f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove blank entries\n",
    "df.drop(blanks, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a81c3e88-8c37-45a9-9515-fa200e08c3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1938"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final number of items\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "36c8407e-a124-4ab1-b9a9-64956fe16da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neg    969\n",
       "pos    969\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Is the dataset balanced? Target is 50/50, great!\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0196e8-b856-46e1-9b52-722a095fe8a9",
   "metadata": {},
   "source": [
    "## 2. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "673f910f-7c30-48ea-aed8-3db7265672ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['review']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d952d7a-0868-41f3-a14f-073f22935f58",
   "metadata": {},
   "source": [
    "## 3. Build Pipelines: Naive Bayes & Support Vector Machines with `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e5ff4a-684e-43aa-88ca-fbf8b2101a59",
   "metadata": {},
   "source": [
    "When building our `Pipeline`, we can pass the **stop words** to the `TfidfVectorizer`:\n",
    "- `TfidVectorizer(stop_words='english')` to accept scikit-learn's built-in list,\n",
    "- or `TfidVectorizer(stop_words=[a, and, the])` to accept a custom list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "23e5044c-0dee-42c6-9cb0-f673f060113f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'con', 'hundred', 'behind', 'everyone', 'least', 'namely', 'as', 'always', 'would', 'hasnt', 'until', 'again', 'back', 'toward', 'empty', 'may', 'eg', 'all', 'none', 'eleven', 'found', 'per', 'side', 'there', 'therein', 'thereafter', 'seeming', 'very', 'the', 'wherein', 'everywhere', 'up', 'several', 'among', 'interest', 'be', 'mostly', 'has', 'four', 'anyway', 'thereby', 'but', 'whom', 'afterwards', 'than', 'couldnt', 'twenty', 'beforehand', 'out', 'thence', 'amoungst', 'hers', 'latterly', 'too', 'something', 'whenever', 'upon', 'system', 'whether', 'not', 'other', 'cry', 'her', 'seems', 'who', 'call', 'neither', 'some', 'full', 'serious', 'take', 'then', 'yours', 'whither', 'which', 'whoever', 'since', 'what', 'it', 'ever', 'nowhere', 'hereafter', 'sometimes', 'three', 'yet', 'he', 'one', 'less', 'bill', 'often', 'six', 'hereby', 'forty', 'were', 'fill', 'where', 'indeed', 'ie', 'and', 'you', 'few', 'here', 'with', 'of', 'such', 'become', 'hereupon', 'below', 'across', 'beyond', 'describe', 'besides', 'should', 'fifty', 'can', 'twelve', 'off', 'another', 'wherever', 'seem', 'sixty', 'enough', 'seemed', 'why', 'because', 'thin', 'that', 'ourselves', 'although', 'no', 'anything', 'five', 'itself', 'onto', 'throughout', 'nine', 'my', 'much', 'nobody', 'amount', 'became', 'along', 'someone', 'move', 'whereas', 'go', 'more', 'in', 'do', 'elsewhere', 'see', 'therefore', 'or', 'never', 'cant', 'anyone', 'de', 'sincere', 'detail', 'its', 'otherwise', 'cannot', 'me', 'whereupon', 'almost', 'them', 'former', 'over', 'alone', 'bottom', 'once', 'thru', 'our', 'together', 'through', 'further', 'beside', 'keep', 'well', 'already', 'when', 'both', 'she', 'even', 'whereby', 'made', 'after', 'their', 'yourself', 'formerly', 'against', 'inc', 'might', 'becomes', 'by', 'i', 'third', 'first', 'they', 'on', 'front', 'us', 'own', 'about', 'an', 'without', 'either', 'will', 'via', 'every', 'next', 'thus', 'find', 'everything', 'down', 'before', 'co', 'how', 'hence', 'we', 'done', 'within', 'had', 'somehow', 'around', 'eight', 'ltd', 'un', 'mine', 'else', 'part', 'mill', 'above', 'your', 'am', 'whatever', 'is', 'due', 'still', 'have', 'must', 'at', 'now', 'name', 'top', 'yourselves', 'two', 'herself', 'any', 'meanwhile', 'anywhere', 'only', 'thick', 'to', 'if', 'ours', 'same', 'etc', 'give', 'could', 'himself', 'noone', 'this', 'fifteen', 'towards', 'get', 'for', 'rather', 'was', 'during', 'thereupon', 'under', 'are', 'last', 'whereafter', 'those', 'herein', 'nor', 'nothing', 'sometime', 'from', 'except', 'his', 'perhaps', 'themselves', 'each', 'fire', 'whose', 'though', 'into', 'please', 'moreover', 'put', 'him', 'amongst', 'however', 'whole', 'others', 'nevertheless', 'a', 'ten', 'anyhow', 'also', 'show', 'latter', 'been', 'many', 'becoming', 'most', 'somewhere', 'being', 'between', 'so', 'myself', 're', 'these', 'whence', 'while'})\n"
     ]
    }
   ],
   "source": [
    "# List of default stop words in scikit-learn\n",
    "from sklearn.feature_extraction import text\n",
    "print(text.ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b6c48a2d-a283-4507-91de-06b4f7d10e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Na√Øve Bayes:\n",
    "text_clf_nb = Pipeline([('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "                     ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Linear SVC:\n",
    "text_clf_lsvc = Pipeline([('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "                     ('clf', LinearSVC()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656dbf78-9044-4ce9-9e7f-92c381518b3d",
   "metadata": {},
   "source": [
    "## 4. Evaluate Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dc55a0-ca60-43c9-8abe-f258e5adfc6e",
   "metadata": {},
   "source": [
    "### 4.1 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cc1ab080-d818-4801-b19f-c6000250dba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer(stop_words='english')),\n",
       "                ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5b27b9b0-d01b-43e0-9127-4e6617d3cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form a prediction set\n",
    "predictions = text_clf_nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d026cd13-19e2-4a88-8e87-cf428725a635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[274  34]\n",
      " [ 94 238]]\n"
     ]
    }
   ],
   "source": [
    "# Report the confusion matrix\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fdda0c56-0da8-4bcb-b618-ee665e27dbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.74      0.89      0.81       308\n",
      "         pos       0.88      0.72      0.79       332\n",
      "\n",
      "    accuracy                           0.80       640\n",
      "   macro avg       0.81      0.80      0.80       640\n",
      "weighted avg       0.81      0.80      0.80       640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a classification report\n",
    "print(metrics.classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "98d40672-a536-459c-b798-80645a235691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    }
   ],
   "source": [
    "# Print the overall accuracy\n",
    "print(metrics.accuracy_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71612b0d-5ab0-414e-bfa9-a01dbb9ed33e",
   "metadata": {},
   "source": [
    "### 4.2 Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9fb09db0-a643-48da-871c-ac24bf6be2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer(stop_words='english')),\n",
       "                ('clf', LinearSVC())])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_lsvc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2058319a-6127-4b70-987e-d3bb9b02ed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form a prediction set\n",
    "predictions = text_clf_lsvc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "56241e64-6a1e-4230-a5a6-49d44af03348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[252  56]\n",
      " [ 52 280]]\n"
     ]
    }
   ],
   "source": [
    "# Report the confusion matrix\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3f375b6d-e6ab-4ca3-bfd7-aa0e2554bd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.83      0.82      0.82       308\n",
      "         pos       0.83      0.84      0.84       332\n",
      "\n",
      "    accuracy                           0.83       640\n",
      "   macro avg       0.83      0.83      0.83       640\n",
      "weighted avg       0.83      0.83      0.83       640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a classification report\n",
    "print(metrics.classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "34525178-8e42-4bd1-a9cf-160f9a7d6b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83125\n"
     ]
    }
   ],
   "source": [
    "# Print the overall accuracy\n",
    "print(metrics.accuracy_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ffedb1-6c4d-4c28-b8ee-e14f4f31060e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
