{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bdea501-bd1e-4a09-b4a7-10eda30c5fba",
   "metadata": {},
   "source": [
    "# Semantics and Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172dfc10-c1df-4927-8861-317cc3949d10",
   "metadata": {},
   "source": [
    "We are going to use **embedded word vectors** already available in Spacy; but for that, we need to use medium or large language models, which need to be installed explicitly:\n",
    "\n",
    "```bash\n",
    "python -m spacy download en_core_web_md # spacy.load('en_core_news_md')\n",
    "python -m spacy download en_core_web_lg # spacy.load('en_core_news_lg')\n",
    "```\n",
    "\n",
    "When we load any of these models, each token has its vector representation. The concept of representing words with vectors was popularized by Mikolov et al. in 2013 (Google) -- see `../literature/Mikolov_Word2Vec_2013.pdf`.\n",
    "\n",
    "The idea is that we get an `N` dimensional vector representation of each word in the vocabulary, such that:\n",
    "- **Close vectors are words semantically related**, and associations can be inferred: `man` is to `boy` as `woman` is to `girl`.\n",
    "- **We can perform vector operations that are reflected in the semantical space**: `vector(queen) ~ vector(king) - vector(man) + vector(woman)`.\n",
    "\n",
    "In order to generate those word vector embeddings, large corpuses of texts are trained with sets of close words mapping each word to a numerical vector. I understand that in the begining, words are represented as one-hot encoded vectors of dimension `M`, being `M` the size of the vocabulary:\n",
    "\n",
    "`[0, 1, 0, ..., 0] (M: vocabulary size) -> [0.2, 0.5, ..., 0.1] (N: latent word vector space)`\n",
    "\n",
    "A common metric to measure similarity between word vectors is the **cosine similarity**: cosine of the angle formed by the two words.\n",
    "\n",
    "In Spacy, a word vector has dimension `N = 300`; however, not all language models have word vectors!\n",
    "- `en_core_news_sm` (35MB): no word vector representations\n",
    "- `en_core_news_md` (116MB): 685k keys, 20k unique vectors (300 dimensions)\n",
    "- `en_core_news_lg` (812MB): 685k keys, 685k unique vectors (300 dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1206a3e-618c-4590-84bd-24a7cab2f105",
   "metadata": {},
   "source": [
    "Overview of contents:\n",
    "\n",
    "1. Word Vectors: Token & Doc Vectors\n",
    "2. Vector Similarity: Cosine Similarity\n",
    "3. Vector Norms\n",
    "4. Vector Arithmetic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5cc474-888e-4ee0-90a7-130ec1f6d914",
   "metadata": {},
   "source": [
    "*Diclaimer: I made this notebook while following the Udemy course [NLP - Natural Language Processing with Python](https://www.udemy.com/course/nlp-natural-language-processing-with-python/) by Jos√© Marcial Portilla. The original course notebooks and materials were provided with a download link, I haven't found a repository to fork from.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f89b5f-096a-447a-9fd8-eee53f2ec3b8",
   "metadata": {},
   "source": [
    "## 1. Word Vectors: Token & Doc Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "613749af-3419-4da7-ba6a-835ffc24a7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "#nlp = spacy.load('en_core_web_md')  # make sure to use a larger model - it takes longer\n",
    "nlp = spacy.load('en_core_web_lg')  # make sure to use a larger model - it takes longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "54b95355-ac93-4697-a611-162d55f3366a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(u'lion').vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "43e7d7cb-8797-44c5-a01c-c104089fabd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.8963e-01, -4.0309e-01,  3.5350e-01, -4.7907e-01, -4.3311e-01,\n",
       "        2.3857e-01,  2.6962e-01,  6.4332e-02,  3.0767e-01,  1.3712e+00,\n",
       "       -3.7582e-01, -2.2713e-01, -3.5657e-01, -2.5355e-01,  1.7543e-02,\n",
       "        3.3962e-01,  7.4723e-02,  5.1226e-01, -3.9759e-01,  5.1333e-03,\n",
       "       -3.0929e-01,  4.8911e-02, -1.8610e-01, -4.1702e-01, -8.1639e-01,\n",
       "       -1.6908e-01, -2.6246e-01, -1.5983e-02,  1.2479e-01, -3.7276e-02,\n",
       "       -5.7125e-01, -1.6296e-01,  1.2376e-01, -5.5464e-02,  1.3244e-01,\n",
       "        2.7519e-02,  1.2592e-01, -3.2722e-01, -4.9165e-01, -3.5559e-01,\n",
       "       -3.0630e-01,  6.1185e-02, -1.6932e-01, -6.2405e-02,  6.5763e-01,\n",
       "       -2.7925e-01, -3.0450e-03, -2.2400e-02, -2.8015e-01, -2.1975e-01,\n",
       "       -4.3188e-01,  3.9864e-02, -2.2102e-01, -4.2693e-02,  5.2748e-02,\n",
       "        2.8726e-01,  1.2315e-01, -2.8662e-02,  7.8294e-02,  4.6754e-01,\n",
       "       -2.4589e-01, -1.1064e-01,  7.2250e-02, -9.4980e-02, -2.7548e-01,\n",
       "       -5.4097e-01,  1.2823e-01, -8.2408e-02,  3.1035e-01, -6.3394e-02,\n",
       "       -7.3755e-01, -5.4992e-01,  9.9999e-02, -2.0758e-01, -3.9674e-02,\n",
       "        2.0664e-01, -9.7557e-02, -3.7092e-01,  2.7901e-01, -6.2218e-01,\n",
       "       -1.0280e-01,  2.3271e-01,  4.3838e-01,  3.2445e-02, -2.9866e-01,\n",
       "       -7.3611e-02,  7.1594e-01,  1.4241e-01,  2.7770e-01, -3.9892e-01,\n",
       "        3.6656e-02,  1.5759e-01,  8.2014e-02, -5.7343e-01,  3.5457e-01,\n",
       "        2.2491e-01, -6.2699e-01, -8.8106e-02,  2.4361e-01,  3.8533e-01,\n",
       "       -1.4083e-01,  1.7691e-01,  7.0897e-02,  1.7951e-01, -4.5907e-01,\n",
       "       -8.2120e-01, -2.6631e-02,  6.2549e-02,  4.2415e-01, -8.9630e-02,\n",
       "       -2.4654e-01,  1.4156e-01,  4.0187e-01, -4.1232e-01,  8.4516e-02,\n",
       "       -1.0626e-01,  7.3145e-01,  1.9217e-01,  1.4240e-01,  2.8511e-01,\n",
       "       -2.9454e-01, -2.1948e-01,  9.0460e-01, -1.9098e-01, -1.0340e+00,\n",
       "       -1.5754e-01, -1.1964e-01,  4.9888e-01, -1.0624e+00, -3.2820e-01,\n",
       "       -1.1232e-02, -7.9482e-01,  3.7275e-01, -6.8710e-03, -2.5772e-01,\n",
       "       -4.7005e-01, -4.1387e-01, -6.4089e-02, -2.8033e-01, -4.0778e-02,\n",
       "       -2.4866e+00,  6.2494e-03, -1.0210e-02,  1.2752e-01,  3.4965e-01,\n",
       "       -1.2571e-01,  3.1570e-01,  4.1926e-01,  2.0056e-01, -5.5984e-01,\n",
       "       -2.2801e-01,  1.2012e-01, -2.0518e-03, -8.9764e-02, -8.0373e-02,\n",
       "        1.1969e-02, -2.6978e-01,  3.4829e-01,  7.3664e-03, -1.1137e-01,\n",
       "        6.3410e-01,  3.8449e-01, -6.2248e-01,  4.1145e-02,  2.5922e-01,\n",
       "        6.5811e-01, -4.9548e-01, -1.3030e-01, -3.8279e-01,  1.1156e-01,\n",
       "       -4.3085e-01,  3.4473e-01,  2.7109e-02, -2.5108e-01, -2.8011e-01,\n",
       "        2.1662e-01,  3.2660e-01,  5.5895e-02,  7.6077e-02, -5.2480e-02,\n",
       "        4.5928e-02, -2.5266e-01,  5.2845e-01, -1.3145e-01, -1.2453e-01,\n",
       "        4.0556e-01,  3.1877e-01,  2.4415e-02, -2.2620e-01, -6.1960e-01,\n",
       "       -4.0886e-01, -3.5534e-02, -5.5123e-03,  2.3438e-01,  8.7854e-01,\n",
       "       -2.5161e-01,  4.0600e-01, -4.4284e-01,  3.4934e-01, -5.6429e-01,\n",
       "       -2.3676e-01,  6.2199e-01, -2.8175e-01,  4.2024e-01,  1.0043e-01,\n",
       "       -1.4720e-01,  4.9593e-01, -3.5850e-01, -1.3998e-01, -2.7494e-01,\n",
       "        2.3827e-01,  5.7268e-01,  7.9025e-02,  1.7872e-02, -2.1829e-01,\n",
       "        5.5050e-02, -5.4200e-01,  1.6788e-01,  3.9065e-01,  3.0209e-01,\n",
       "        2.3040e-01, -3.9351e-02, -2.1078e-01, -2.7224e-01,  1.6907e-01,\n",
       "        5.4819e-01,  9.4888e-02,  7.9798e-01, -6.6158e-02,  1.9844e-01,\n",
       "        2.0307e-01,  4.4808e-02, -1.0240e-01, -6.9909e-02, -3.6756e-02,\n",
       "        9.5159e-02, -2.7830e-01, -1.0597e-01, -1.6276e-01, -1.8211e-01,\n",
       "       -3.1897e-01, -2.1633e-01,  1.4994e-01, -7.2057e-02,  2.2264e-01,\n",
       "       -4.5551e-01,  3.0341e-01,  1.8431e-01,  2.1681e-01, -3.1940e-01,\n",
       "        2.6426e-01,  5.8106e-01,  5.4635e-02,  6.3238e-01,  4.3169e-01,\n",
       "        9.0343e-02,  1.9494e-01,  3.5483e-01, -2.0706e-02, -7.3117e-01,\n",
       "        1.2941e-01,  1.7418e-01, -1.5065e-01,  5.3355e-02,  4.4794e-02,\n",
       "       -1.6600e-01,  2.2007e-01, -5.3970e-01, -2.4968e-01, -2.6464e-01,\n",
       "       -5.5515e-01,  5.8242e-01,  2.2295e-01,  2.4433e-01,  4.5275e-01,\n",
       "        3.4693e-01,  1.2255e-01, -3.9059e-02, -3.2749e-01, -2.7891e-01,\n",
       "        1.3766e-01,  3.8392e-01,  1.0543e-03, -1.0242e-02,  4.9205e-01,\n",
       "       -1.7922e-01,  4.1215e-02,  1.3547e-01, -2.0598e-01, -2.3194e-01,\n",
       "       -7.7701e-01, -3.8237e-01, -7.6383e-01,  1.9418e-01, -1.5441e-01,\n",
       "        8.9740e-01,  3.0626e-01,  4.0376e-01,  2.1738e-01, -3.8050e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(u'lion').vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "aa90a30b-a90e-4650-bab9-19677af429cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "684830"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of unique vectors loaded in the model\n",
    "len(nlp.vocab.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "34a3fe31-81ce-4494-87a3-09e227be948d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doc and Span objects themselves have vectors,\n",
    "# derived from the averages of individual token vectors. \n",
    "# This makes it possible to compare similarities between whole documents.\n",
    "doc1 = nlp(u'The quick brown fox jumped over the lazy dogs.')\n",
    "doc1.vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "352cae5d-fc90-4d14-921b-05a21941e509",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = doc1.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2b8ed4fb-56e3-4760-9eaf-69120df8ba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to the documentation, the vector of a Doc is averaged\n",
    "# without considering the position of each word.\n",
    "# However, there seems to be some positional encoding, because the vectors are not the same\n",
    "# Or is it just the numerical error?\n",
    "#doc = nlp(u'The quick brown jumped fox over the lazy dogs.')\n",
    "doc2 = nlp(u'The brown quick jumped fox over the lazy dogs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f2e16e5e-395e-479f-b956-0a1de20aeb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "v2 = doc.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3e8a560f-f2b4-457b-9a8f-733ea62136ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1=v1-v1\n",
    "d2=v1-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7ae1a5b1-c9fc-4786-8944-19a0c0629a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(sum(d1*d1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2e0180d7-818c-457f-8a59-f492d7d5d616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.607210729790503e-08"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(sum(d2*d2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a517c9ff-de3d-42b3-a55d-5f6e49e9fa63",
   "metadata": {},
   "source": [
    "## 2. Vector Similarity: Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3f1c32d8-d51b-4c32-bcca-c67e68ed89b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lion lion 1.0\n",
      "lion cat 0.5265437\n",
      "lion pet 0.39923766\n",
      "cat lion 0.5265437\n",
      "cat cat 1.0\n",
      "cat pet 0.7505457\n",
      "pet lion 0.39923766\n",
      "pet cat 0.7505457\n",
      "pet pet 1.0\n"
     ]
    }
   ],
   "source": [
    "# Create a three-token Doc object\n",
    "tokens = nlp(u'lion cat pet')\n",
    "\n",
    "# Iterate through token combinations\n",
    "# Note: token1.similarity(token2) == token2.similarity(token1)\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1d3ccebf-d087-4c9c-b459-fafd6bfefef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like like 1.0\n",
      "like love 0.657904\n",
      "like hate 0.65746516\n",
      "love like 0.657904\n",
      "love love 1.0\n",
      "love hate 0.63930994\n",
      "hate like 0.65746516\n",
      "hate love 0.63930994\n",
      "hate hate 1.0\n"
     ]
    }
   ],
   "source": [
    "# Opposites are not necessarily different!\n",
    "tokens = nlp(u'like love hate')\n",
    "\n",
    "# Iterate through token combinations:\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0907268b-876a-4aca-8d8d-7177bc0bc96e",
   "metadata": {},
   "source": [
    "## 3. Vector Norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ab040178-ecd7-4dfa-a5fe-aa333cb9ce59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "684830"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of unique vectors loaded in the model\n",
    "len(nlp.vocab.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d71b24-1034-4843-aff7-dc43959d5529",
   "metadata": {},
   "source": [
    "Note that usual words, including names, can have vector representations; however, in some cases we can come up with a word that has no vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "270c2520-1637-40f8-9932-8ac03350a67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog True 7.0336733 False\n",
      "cat True 6.6808186 False\n",
      "nargle False 0.0 True\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp(u'dog cat nargle')\n",
    "# token.has_vector: True/False\n",
    "# token.vector_norm: L2 norm or Euclidean length of the vector\n",
    "# token.is_oov: is out-of-vocabulary, True/False (maybe it is in vocabulary, but has no vector)\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1938342f-cafb-40c1-8549-136a1414482b",
   "metadata": {},
   "source": [
    "## 4. Vector Arithmetic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb4c37e-5cb3-4355-a8db-48ca1f16c0f1",
   "metadata": {},
   "source": [
    "With word vector embeddings we can perform arithmetics that are reflected in meaningful sematic operations:\n",
    "\n",
    "`vector(queen) ~ vector(king) - vector(man) + vector(woman)`\n",
    "\n",
    "However, in my case at least, it does not seem to work that well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7fa8ff39-a2a9-4926-b0c2-52a8ac00aba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['king', 'monarch', 'woman', 'female', 'she', 'lion', 'male', 'who', 'fox', 'brown']\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "# Our custom similarity function\n",
    "cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n",
    "\n",
    "king = nlp.vocab['king'].vector\n",
    "man = nlp.vocab['man'].vector\n",
    "woman = nlp.vocab['woman'].vector\n",
    "\n",
    "# Now we find the closest vector in the vocabulary to the result of\n",
    "# \"king\" - \"man\" + \"woman\"\n",
    "new_vector = king - man + woman\n",
    "computed_similarities = []\n",
    "\n",
    "# Visit all words/tokens in the vocabulary\n",
    "# and if they have a valid vector, compute the similarity\n",
    "for word in nlp.vocab:\n",
    "    # Ignore words without vectors and mixed-case words\n",
    "    if word.has_vector:\n",
    "        if word.is_lower:\n",
    "            if word.is_alpha:\n",
    "                similarity = cosine_similarity(new_vector, word.vector)\n",
    "                computed_similarities.append((word, similarity))\n",
    "\n",
    "# Sort all similarities by first time, descending\n",
    "computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\n",
    "\n",
    "# Unfortunately, it does not seem to work that well...\n",
    "print([w[0].text for w in computed_similarities[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9964046f-1fa7-443f-91c6-6f71b5bbe9f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
