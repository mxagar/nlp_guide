{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfc55aac-4cf6-41f3-a4e4-f6397d3166bc",
   "metadata": {},
   "source": [
    "# Topic Modeling: Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae6e851-7d78-4b0f-9d8c-02a458d63ae7",
   "metadata": {},
   "source": [
    "This notebook introduces the concept of **Latent Dirichlet Allocation (LDA)**, which is an essential unsupervised learning technique to approach **topic modeling**.\n",
    "\n",
    "In topic modeling, we typically have large amounts of **unlabeled** data/text (corpus) divided in many documents. The goal is to cluster those documents into topic-groups, which need to be discovered, i.e., we don't know the topic contents, since they are to be detected by the approach.\n",
    "\n",
    "The [LDA](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) approach was published in 2003 by Blei, Ng & Jordan:\n",
    "\n",
    "`../literature/BleiNgJordan_LDA_2003.pdf`\n",
    "\n",
    "The method uses the [Dirichlet distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution), hence the name -- not that they invented the distirbution. The Dirichlet distribution is a multivariate distribution with the property that the sum of all its variables needs to be 1.\n",
    "\n",
    "These **assumptions** are done in LDA:\n",
    "\n",
    "- Documents with similar topics use similar groups of words.\n",
    "- Latent topics can be found by searching for groups of words that frequently occur together in the documents.\n",
    "\n",
    "These assumptions are translated into probability distributions:\n",
    "\n",
    "- Documents are probability distributions over `K` latent topics; these latent topics are like bins, and for each document, the weight of dealing with any of the topics is discovered.\n",
    "- Topics are probability distributions over words. The idea is anallogous to the previous point. In practice, once the topic has been discovered, we check the top-10 words from its topic-word distribution and infer what the theme is.\n",
    "\n",
    "![LDA Topic-Document-Word Distributions](../pics/LDA_distributions.png)\n",
    "\n",
    "LDA starts working as follows:\n",
    "\n",
    "- We have `M` documents, each document `d` with `N_d` words in it, from a corpus vocabulary consisting of `W` words.\n",
    "- We set a fixed amount of latent topics `K` that are going to be discrovered, as the number of clusters in K-means; e.g., `K = 50`.\n",
    "- We assign (randomly) to each document the weights/percentages associated to each topic: `alpha_k`, `k = 1:K`, `sum(alpha_k) = 1`.\n",
    "- From the random assignment done for document-topics, we get the topic weights/percentages associated to each word: `beta_w`, `w = 1:W`, `sum(beta_w) = 1`.\n",
    "\n",
    "Notes on the initialization:\n",
    "- The LDA approach assumes that the documents are generated following the points below; although it's not true, it's a useful construct that.\n",
    "- The first assignment does not make any sense, but we iterate to improve it.\n",
    "\n",
    "Once we have performed the initialization, the optimization algorithm works as follows:\n",
    "1. We iterate over every **word** in every **document**, and for each **topic** we compute:\n",
    "    - `p(topic k | document d) = proportion of words in document d that are assigned to topic k`.\n",
    "    - `p(word w | topic k) = proportion of assignments to topic k over all documents that contain/come this/from word w`.\n",
    "2. We re-assign each **word** a new **topic**, where we choose topic k with this probability:\n",
    "    - `p(topic k | document d) * p(word w | topic k): probability that topic k is generated from word w`.\n",
    "    - Document topics are re-computed after the re-assignment of word-topic probabilities.\n",
    "3. We repeat these steps 1 & 2 enough times until we reach a steady state.\n",
    "\n",
    "Note that before applying anything it is convenient to (1) remove stop-words and (2) reduce the tokens/words to a base form using stemming or lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b64dac9-4fa2-4623-836d-72a37fa9dbf3",
   "metadata": {},
   "source": [
    "Overview of contents:\n",
    "1. Load the Dataset\n",
    "2. Create a Document-Term Matrix (DTM) and Fit the Latent Dirichlet Allocation (LDA) Model to It"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a865121f-394f-4da8-998b-3dfaf9685cf8",
   "metadata": {},
   "source": [
    "*Diclaimer: I made this notebook while following the Udemy course [NLP - Natural Language Processing with Python](https://www.udemy.com/course/nlp-natural-language-processing-with-python/) by José Marcial Portilla. The original course notebooks and materials were provided with a download link, I haven't found a repository to fork from.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2a05af-b4a5-4a23-ab39-9469af301894",
   "metadata": {},
   "source": [
    "## 1. Load the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0013fced-cc96-4cef-916c-e007a6902119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4db84883-d287-4136-8e03-f92ca542eac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the NPR dataset: around 12k articles; we want to discover and assign topics\n",
    "npr = pd.read_csv('../data/npr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f4deb14-44ff-4588-9251-ee54f26932de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11992, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57db2d06-7649-442a-bf4e-a4a4ff5ae95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the Washington of 2016, even when the polic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump has used Twitter  —   his prefe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump is unabashedly praising Russian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From photography, illustration and video, to d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article\n",
       "0  In the Washington of 2016, even when the polic...\n",
       "1    Donald Trump has used Twitter  —   his prefe...\n",
       "2    Donald Trump is unabashedly praising Russian...\n",
       "3  Updated at 2:50 p. m. ET, Russian President Vl...\n",
       "4  From photography, illustration and video, to d..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5e11256-bec6-4eb7-8e8f-5112ddef7988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For years now, some of the best, wildest, most moving or revealing stories we’ve been telling ourselves have come not from books, movies or TV, but from video games. So we’re running an occasional series, Reading The Game, in which we take a look at some of these games from a literary perspective. I played the game through the first time in something like a perfect state of awe and terror. Enraptured is, I think, the word that best describes it. Carried away completely into this ruined, beautiful world and the story of Joel and Ellie in The Last of Us. Normally such a completionist  —   so obsessed with exploring every hide and hollow in these imaginary worlds I throw myself into  —   in this instance I simply rolled with the narrative. Ran when running was proper. Slogged through dark and rain and snow and sunshine. Stood my bloody ground when left with no other options. Joel came to love Ellie, his surrogate daughter, and Ellie came to love Joel, the only father she’d ever known. And I (a father, with a daughter roughly Ellie’s age, with Ellie’s   vocabulary and Ellie’s strange, discordant humor) loved Ellie, too. So when I reached the endgame and was presented with a terrible choice (no spoilers . .. yet) I drew my guns and slaughtered my way to the end credits, alight with fury and sure knowledge that I’d made the only choice I could. Second run: The beats are all the same, the story a known thing. Joel and Ellie fight zombies and soldiers and bandits and madmen. They lose friends and see sunrises and, this time, I play with an awful wisdom. Cassandra’s curse. I know how this story ends and I have made up my mind that, this time, I will make the other choice. The right one (morally, mathematically, humanistically) and so I walk with ghosts the whole way, right up to the end, and then . .. And then I make the exact same choice again. I can’t make the other. It hurts too much. Because that is how good the storytelling is in The Last Of Us. It makes you care so deeply for a   bunch of pixels in the shape of a teenage girl that you will damn the whole world twice just for her. (OK, so now we’re gonna get spoilery. Fair warning.) The Last Of Us is a zombie story. It is incredibly derivative, borrows liberally from a hundred different books and movies, is structurally simplistic,   melodramatic, viscerally violent, and despite all this (or, arguably, because of all this) tells one of the most moving, affecting and satisfying stories you’ll find anywhere. At its heart, it is the story of Joel  —   a broken and   thief and smuggler living 20 years deep into a zombie apocalypse. He and his partner, Tess, are forced into a job that requires them to smuggle a young girl out of the Boston quarantine zone and deliver her to an army of revolutionaries because, of course, this girl is The One  —   the only person ever to be immune to the   that turns infected people into gross, murderous mushroom zombies. That young girl is Ellie. And, unsurprisingly, the job does not exactly go as planned. If this all sounds familiar, that’s fine because it is familiar. The   is a stock frame  —   tested and dependable. It is a road trip story in the same way that Cormac McCarthy’s The Road is, or Mad Max: Fury Road. Go from point A to point B, survive the journey, get there whole. And there’s nothing at all wrong with a simple narrative architecture when it is being used to support complex character arcs, as it is here. The Last Of Us is a simple road trip story underneath, existing in service to the complex and rich redemption story on top. All the stakes and ruination are laid out in the first 10 minutes, in a prologue so powerful that it’ll break your heart even if you don’t have one. Joel loses his daughter on the night the world ends, his little girl dying in his arms, under the gun of a panicked soldier trying to hold back the infected. When Ellie floats into his life two decades later, the jaded gamer in you says, Oh, so here’s where he learns to love again. . .. And you’re right. But then you watch it happen  —   in tiny moments like when Ellie, blowing off caution, walks a rickety plank between two buildings and Joel glances briefly down at the watch he wears, a gift from his daughter that he’s been wearing for 20 years  —   and you participate in it happening (protecting her, defending her, eventually becoming her for an extended chunk of the game in a brilliant bit of perspective switching) and it all just clicks. This is a love story  —   one of the best    narratives ever told. Which is when that ending comes and you are presented with the ultimate parental nightmare scenario: Will you sacrifice the life of your child to save the world? Not a stranger, a friend or even a spouse, but your own daughter (which is what Ellie is now  —   Joel’s daughter, blood or no). Because in Ellie lives the cure to the mushroom zombie plague. But in order to create it, she has to die. I started a third playthrough before writing this piece. I am walking slow, taking my time, listening to Ellie read from her joke book, watching her swarmed by fireflies on the outskirts of Boston and admiring the natural beauty and deep environmental storytelling of the game. Nature has reclaimed most of this abandoned world, giving us an unusual apocalypse run riot with wildflowers. And while I have not made it to the end yet, I know it’s coming. I know the choice I’m going to have to make. And I know exactly what I’m going to do. Jason Sheehan is an   a former restaurant critic and the current food editor of Philadelphia magazine. But when no one is looking, he spends his time writing books about spaceships, aliens, giant robots and ray guns. Tales From the Radiation Age is his latest book.\n"
     ]
    }
   ],
   "source": [
    "# Text of an article i\n",
    "i = 10\n",
    "print(npr[\"Article\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b66d007-5d0c-4d1b-b49d-07814ac76553",
   "metadata": {},
   "source": [
    "## 2. Create a Document-Term Matrix (DTM) and Fit the Latent Dirichlet Allocation (LDA) Model to It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fee6c617-4787-4467-80f6-b46923bafce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "563cc088-1769-4847-8a3f-e8d21b645fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important parameters of CountVectorizer\n",
    "# max_df: When building the vocabulary \n",
    "#   ignore terms that have a document frequency strictly higher than the given threshold,\n",
    "#   i.e., corpus-specific stop words.\n",
    "#   If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "# min_df: When building the vocabulary\n",
    "#   ignore terms that have a document frequency strictly lower than the given threshold.\n",
    "#   This value is also called cut-off in the literature.\n",
    "#   If float, the parameter represents a proportion of documents, integer absolute counts\n",
    "# Stop words: we remove them\n",
    "cv = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82882f87-4565-4afc-a8dc-8a5f72b3f1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build the Document-Term Matrix\n",
    "# We can't do any split, because that's unsupervised learning!\n",
    "dtm = cv.fit_transform(npr['Article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5daeb8c-1421-4198-be6c-888169683240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00af8e33-c0ab-416e-bed4-017688f9309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent Dirichlet Allocation\n",
    "# n_components: number of topics\n",
    "LDA = LatentDirichletAllocation(n_components=7,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d9ee1af-98d7-49af-b416-7024e7353a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=7, random_state=42)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We fit Latent Dirichlet Allocation model to our Document-Term Matrix\n",
    "# This can take awhile, we're dealing with a large amount of documents!\n",
    "LDA.fit(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d183b19-9452-4f4b-9566-440c47a73a6c",
   "metadata": {},
   "source": [
    "## 3. Explore the Discovered Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "948179f5-9e29-45ea-978f-7297fb486950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54777"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all words in the DTM, i.e., our vocabulary\n",
    "len(cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8eb8186-ad80-447d-8e7b-29eae05fb02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks\n",
      "rehabbed\n",
      "baffled\n",
      "patti\n",
      "tiered\n",
      "repayment\n",
      "vladmir\n",
      "leil\n",
      "weissman\n",
      "discreet\n"
     ]
    }
   ],
   "source": [
    "# Explore some of those vocabulary words\n",
    "import random\n",
    "for i in range(10):\n",
    "    random_word_id = random.randint(0,54776)\n",
    "    print(cv.get_feature_names()[random_word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db97548d-e3a2-471a-8d85-a969c44c2532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 54777)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix with topic-word weights/probabilities: topics x words\n",
    "LDA.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21fec21f-827a-4b25-864f-a2e5d9942097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a single topic k\n",
    "k = 0\n",
    "single_topic = LDA.components_[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7915860d-39fa-4179-94f4-c1cdba6482f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21349, 37109, 17024, ..., 47210, 43172, 42993])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get indices that sort this array: [0.9 0.7 0.3] -> [2 1 0]\n",
    "# These are the word indices ordered according to their weight for topic k\n",
    "# Watch out the order: ascending (default) / descending\n",
    "single_topic.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a183f4b3-7fce-4c9c-b14f-4bf017d1b46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 15 WORDS FOR TOPIC #1\n",
      "['companies', 'money', 'year', 'federal', '000', 'new', 'percent', 'government', 'company', 'million', 'care', 'people', 'health', 'said', 'says']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #2\n",
      "['military', 'house', 'security', 'russia', 'government', 'npr', 'reports', 'says', 'news', 'people', 'told', 'police', 'president', 'trump', 'said']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #3\n",
      "['way', 'world', 'family', 'home', 'day', 'time', 'water', 'city', 'new', 'years', 'food', 'just', 'people', 'like', 'says']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #4\n",
      "['time', 'new', 'don', 'years', 'medical', 'disease', 'patients', 'just', 'children', 'study', 'like', 'women', 'health', 'people', 'says']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #5\n",
      "['voters', 'vote', 'election', 'party', 'new', 'obama', 'court', 'republican', 'campaign', 'people', 'state', 'president', 'clinton', 'said', 'trump']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #6\n",
      "['years', 'going', 've', 'life', 'don', 'new', 'way', 'music', 'really', 'time', 'know', 'think', 'people', 'just', 'like']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #7\n",
      "['student', 'years', 'data', 'science', 'university', 'people', 'time', 'schools', 'just', 'education', 'new', 'like', 'students', 'school', 'says']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the 15 most significant words\n",
    "# We can see how we could assign topics/themes:\n",
    "# 1: Economy & Finances\n",
    "# 2: Military and Security Affairs\n",
    "# 3: Family & Resouces\n",
    "# 4: Health\n",
    "# 5: Politics and Elections\n",
    "# 6: Lifestyle\n",
    "# 7: Education\n",
    "for index,topic in enumerate(LDA.components_):\n",
    "    print(f'THE TOP 15 WORDS FOR TOPIC #{index+1}')\n",
    "    print([cv.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ebd368-f59e-480e-bba1-bf89d086d2b0",
   "metadata": {},
   "source": [
    "## 4. Assign Topics to Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "edb9b2ad-b2c6-40c3-bc4c-3d2e174fac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to assign a topic to each article, we need to combine\n",
    "# - the DTM matrix: articles x words\n",
    "# - the LDA weights: topics x words\n",
    "# A weighted multiplication yields the desired matrix:\n",
    "# (articles x topics) <- (articles x words) x (words x topics)\n",
    "topic_results = LDA.transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c4db702-5052-4112-9338-106696e73921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11992, 54777)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6d6ed97d-31cd-406c-97de-296901b90289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 54777)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b84dc72-78e4-41bc-b1bd-1f5335ef87c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11992, 7)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "09f3773b-c774-4e8e-839c-02e7ca454f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02, 0.68, 0.  , 0.  , 0.3 , 0.  , 0.  ])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Topic weights / probabilities of a single article\n",
    "d = 0\n",
    "topic_results[d].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "db7127aa-275b-497e-90e5-5dad7eb98394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Topic index with the highest probability / weight\n",
    "topic_results[0].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6769874f-becd-4e39-86e4-256fb3155f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column with most probable topic index per article\n",
    "npr['Topic'] = topic_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "153a1007-eb22-43ad-a60f-45cbda556d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the Washington of 2016, even when the polic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump has used Twitter  —   his prefe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump is unabashedly praising Russian...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From photography, illustration and video, to d...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article  Topic\n",
       "0  In the Washington of 2016, even when the polic...      1\n",
       "1    Donald Trump has used Twitter  —   his prefe...      1\n",
       "2    Donald Trump is unabashedly praising Russian...      1\n",
       "3  Updated at 2:50 p. m. ET, Russian President Vl...      1\n",
       "4  From photography, illustration and video, to d...      2"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05269309-caba-43e6-9f43-73d282c192dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
