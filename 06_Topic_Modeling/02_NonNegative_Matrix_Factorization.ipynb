{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ed8a84a-428e-4d45-833b-4c8063a7e99b",
   "metadata": {},
   "source": [
    "# Topic Modeling: Non-Negative Matrix Factorization (NNMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ffaa1f-790a-42dc-a8df-93aa118063fa",
   "metadata": {},
   "source": [
    "This notebook shows how topic discovery and assignment can be done using **non-negative matrix factorization (NNMF)**. The idea is equivalent to **collaborative filtering applied in recommender systems**.\n",
    "\n",
    "For an introduction to recommender systems with matrix factorization:\n",
    "- Visit my [Github guide](https://github.com/mxagar/machine_learning_coursera/tree/main/07_Anomaly_Recommender) done after following the Coursera/Stanford course [Machine Learning](https://www.coursera.org/learn/machine-learning) by Andrew Ng.\n",
    "- Have a look at the summary notes in `./RecommenderSystems_Notes.pdf`\n",
    "\n",
    "The key idea is that a Matrix `A (n x m)` is decomposed as the mulziplication of lower rank matrices `W (n x k)` and `H (k x m)`, such that the difference `A - W*H` is minimum accross all elements; i.e., ideally `A = W*H`.\n",
    "\n",
    "![Matrix Factorization](../pics/nnmf_decompostion.png)\n",
    "\n",
    "The elements in the equations are the following:\n",
    "\n",
    "- `A` is the Document-Term Matrix (data: vectorized documents)\n",
    "    - `n` (rows): documents\n",
    "    - `m` (cols): words\n",
    "- `k`: latent topics, number chosen by us\n",
    "- `W`: `documents x topics`; unknown topic weights/probabilities associated to each document, initialized with random values, to be discovered\n",
    "- `H`: `topics x words`; unknown topic weights/probabilities associated to each word, initialized with random values, to be discovered\n",
    "\n",
    "I think that the main differences with recommender systems are:\n",
    "\n",
    "- Now, we have a full matrix `A`, no missing values are present.\n",
    "- The number of words in the vocabulary is expected to be much larger than the number of movies.\n",
    "\n",
    "Apart from that, the cost to be minimized is equivalent to the one seen in recommender systems, and the values of `W` and `H` are updated similarly:\n",
    "\n",
    "$$ \\min{J} = \\min \\frac{1}{2} \\Vert A - WH \\Vert = \\min \\frac{1}{2} \\sum_{i = 1}^{n} \\sum_{j = 1}^{m} (A_{ij} - (WH)_{ij})^{2}$$\n",
    "\n",
    "Note that in practice, this notebook is almost equivalent to the previous one, dealing with Latent Dirichlet Allocation, being the differences:\n",
    "\n",
    "- The TFIDF matrix is computed with `TfidfVectorizer` instead of the DTM (with `TfidfVectorizer`). I understand that setting a maximum value for each document-word pair is a condition for matrix fatorization, as the 5 stars maximum in movie reviews.\n",
    "- The non-negative matrix decomposition from scikit-learn `NMF` is used on the `TfidfVectorizer` matrix, instead of the `LatentDirichletAllocation`.\n",
    "\n",
    "Thus, read the previous notebook first and then have a look at this one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05497bb-5973-4c29-bd46-10c7baa13388",
   "metadata": {},
   "source": [
    "Overview of contents:\n",
    "\n",
    "1. Load the Dataset\n",
    "2. Create a IFIDF Matrix and Fit the Non-Negative Matrix Factorization (NNMF) Model to It\n",
    "3. Explore the Discovered Topics\n",
    "4. Assign Topics to Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcf96eb-fff8-46b5-9a04-4eec13be649e",
   "metadata": {},
   "source": [
    "*Diclaimer: I made this notebook while following the Udemy course [NLP - Natural Language Processing with Python](https://www.udemy.com/course/nlp-natural-language-processing-with-python/) by JosÃ© Marcial Portilla. The original course notebooks and materials were provided with a download link, I haven't found a repository to fork from.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098983aa-c823-4f8b-a996-0fbdabe8c749",
   "metadata": {},
   "source": [
    "## 1. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b964228-95cf-47ce-8aad-df09bc13ae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c84d8b9-9317-4fe1-a1ad-7d4dc7a8782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the NPR dataset: around 12k articles; we want to discover and assign topics\n",
    "npr = pd.read_csv('../data/npr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb31725-4278-42c1-babe-c85200b36230",
   "metadata": {},
   "source": [
    "## 2. Create a IFIDF Matrix and Fit the Non-Negative Matrix Factorization (NNMF) Model to It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "898eff3d-62f1-4910-85d6-984d1845634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "817eef95-9520-4e39-b4da-d9d46907a723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important parameters of TfidfVectorizer\n",
    "# max_df: When building the vocabulary \n",
    "#   ignore terms that have a document frequency strictly higher than the given threshold,\n",
    "#   i.e., corpus-specific stop words.\n",
    "#   If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "# min_df: When building the vocabulary\n",
    "#   ignore terms that have a document frequency strictly lower than the given threshold.\n",
    "#   This value is also called cut-off in the literature.\n",
    "#   If float, the parameter represents a proportion of documents, integer absolute counts\n",
    "# Stop words: we remove them\n",
    "tfidf = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b73b53bf-fa33-4cc3-ad52-0c5ba1bd6c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build the Document-Term Matrix\n",
    "# We can't do any split, because that's unsupervised learning!\n",
    "dtm = tfidf.fit_transform(npr['Article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d6d2b4d-8f89-4ea0-8051-bba5f290dc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67040510-7d30-496b-967b-ceb7754368e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Negative Matrix Factorization\n",
    "# n_components: number of topics\n",
    "nmf_model = NMF(n_components=7,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f45adbc2-70af-4b1a-9546-c5e8468f77b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mxagar/opt/anaconda3/envs/ds/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:294: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(n_components=7, random_state=42)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We fit NMF model to our Document-Term Matrix\n",
    "# This can take awhile, we're dealing with a large amount of documents!\n",
    "nmf_model.fit(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7c6cf1-252f-4515-8b93-18b6bfede74b",
   "metadata": {},
   "source": [
    "## 3. Explore the Discovered Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a4da2d2-5d42-4dc1-949a-80583a0b5afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54777"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all words in the DTM, i.e., our vocabulary\n",
    "len(tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e6eb538-b433-4f57-a253-fffe405c2459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "legislator\n",
      "crusading\n",
      "tells\n",
      "unicorns\n",
      "deleon\n",
      "indictments\n",
      "festivals\n",
      "willie\n",
      "disquisition\n",
      "grated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mxagar/opt/anaconda3/envs/ds/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Explore some of those vocabulary words\n",
    "import random\n",
    "for i in range(10):\n",
    "    random_word_id = random.randint(0,54776)\n",
    "    print(tfidf.get_feature_names()[random_word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0efb6729-ec26-46fc-8c1d-07d2c8e93d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 54777)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix with topic-word weights/probabilities: topics x words\n",
    "nmf_model.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dcbf5246-7c7a-4513-924f-7b1550b0869d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a single topic k\n",
    "k = 0\n",
    "single_topic = nmf_model.components_[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e1dce5c-f942-4037-8f4c-7e9526521185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0, 27208, 27206, ..., 36283, 54692, 42993])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get indices that sort this array: [0.9 0.7 0.3] -> [2 1 0]\n",
    "# These are the word indices ordered according to their weight for topic k\n",
    "# Watch out the order: ascending (default) / descending\n",
    "single_topic.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1edeb5ca-f828-4301-a5a8-6b07d456ea20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 15 WORDS FOR TOPIC #1\n",
      "['new', 'research', 'like', 'patients', 'health', 'disease', 'percent', 'women', 'virus', 'study', 'water', 'food', 'people', 'zika', 'says']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #2\n",
      "['gop', 'pence', 'presidential', 'russia', 'administration', 'election', 'republican', 'obama', 'white', 'house', 'donald', 'campaign', 'said', 'president', 'trump']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #3\n",
      "['senate', 'house', 'people', 'act', 'law', 'tax', 'plan', 'republicans', 'affordable', 'obamacare', 'coverage', 'medicaid', 'insurance', 'care', 'health']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #4\n",
      "['officers', 'syria', 'security', 'department', 'law', 'isis', 'russia', 'government', 'state', 'attack', 'president', 'reports', 'court', 'said', 'police']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #5\n",
      "['primary', 'cruz', 'election', 'democrats', 'percent', 'party', 'delegates', 'vote', 'state', 'democratic', 'hillary', 'campaign', 'voters', 'sanders', 'clinton']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #6\n",
      "['love', 've', 'don', 'album', 'way', 'time', 'song', 'life', 'really', 'know', 'people', 'think', 'just', 'music', 'like']\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #7\n",
      "['teacher', 'state', 'high', 'says', 'parents', 'devos', 'children', 'college', 'kids', 'teachers', 'student', 'education', 'schools', 'school', 'students']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the 15 most significant words\n",
    "# We can see how we could assign topics/themes, similar as with LDA, in a another order\n",
    "# 1: Health\n",
    "# 2: Politics and Elections\n",
    "# 3: Politics and Elections\n",
    "# 4: Security & International Affairs\n",
    "# 5: Elections\n",
    "# 6: Lifestyle\n",
    "# 7: Education\n",
    "# LDA was\n",
    "# 1: Economy & Finances\n",
    "# 2: Military and Security Affairs\n",
    "# 3: Family & Resouces\n",
    "# 4: Health\n",
    "# 5: Politics and Elections\n",
    "# 6: Lifestyle\n",
    "# 7: Education\n",
    "for index,topic in enumerate(nmf_model.components_):\n",
    "    print(f'THE TOP 15 WORDS FOR TOPIC #{index+1}')\n",
    "    print([tfidf.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87606477-2228-4a59-b8b9-de42bc1a63c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
