{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "783a3d44-001b-40b6-8d48-1cd9ac6e3c5c",
   "metadata": {},
   "source": [
    "# Chat Bots with LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d24e11-b9d3-4db2-ad6d-d3c34136886c",
   "metadata": {},
   "source": [
    "This notebook implements a Question-Answer bot which is able to answer questions given a base story. The [BaBi](https://research.facebook.com/downloads/babi/) dataset from Facebook/Meta is used. Theis dataset is composed with examples consisting of three elemets (Story-Question-Answer), as the following:\n",
    "\n",
    "```\n",
    "1. Story: Jane went to the store. Mike ran to the bedroom.\n",
    "2. Question: Is Mike in the store?\n",
    "3. Answer: No\n",
    "```\n",
    "\n",
    "The built model follows paper by Sukhbaatar et al. is linked in the course:\n",
    "\n",
    "`../literature/SukhbaatarFergus_EndToEndMemoryNetworks_2015.pdf`\n",
    "\n",
    "**Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-To-End Memory Networks, 2015.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f065ba-dd7f-4690-80de-f05faafcc364",
   "metadata": {},
   "source": [
    "Overview of contents:\n",
    "1. End-To-End Memory Networks, by Sukhbaatar et al., 2015\n",
    "2. Load and Prepare the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563c1e2b-0333-45aa-838b-5cba1aa479c3",
   "metadata": {},
   "source": [
    "*Diclaimer: I made this notebook while following the Udemy course [NLP - Natural Language Processing with Python](https://www.udemy.com/course/nlp-natural-language-processing-with-python/) by Jos√© Marcial Portilla. The original course notebooks and materials were provided with a download link, I haven't found a repository to fork from.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238b02e6-a8dc-40a7-ac85-84d3a82b52f1",
   "metadata": {},
   "source": [
    "## 1. End-To-End Memory Networks, by Sukhbaatar et al., 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eef0bf5-0e90-4132-bafe-b3c85d5379e3",
   "metadata": {},
   "source": [
    "Answering questions given context is very challenging, because long-term dependencies need to be addressed in sequential data.\n",
    "\n",
    "Appreaches that use **attention** an **explicit storage** have appeared to solve tackle the problem.\n",
    "\n",
    "The papper presents a RNN in which the recurrence reads from an external memory in multiple hops or computational steps per searched symbol.\n",
    "\n",
    "### Approach\n",
    "\n",
    "The model\n",
    "\n",
    "- takes\n",
    "    - a discrete set of inputs that are store in the memory: `x1, x2, ..., xn`\n",
    "    - a query `q`\n",
    "- outputs: an answer `a`\n",
    "\n",
    "Each of the `x`, `a`, `q` contains symbols that com from a vocabulary/dictionary with `V` words. I understand that `x` is a collection of sequences, while `q` and `a` are supposed to be one sequence each.\n",
    "\n",
    "The model writes all `x` to the memory up to a fixed buffer size and then finds a continuous representation for the `x` and `q` (using embeddings). Then, those continuous representations are processed in several steps/hops to obtain `a`.\n",
    "\n",
    "Each layer has the following components:\n",
    "- The input memory where all sentences/sequences `x` and `q` are stored encoded in an embedding A.\n",
    "- The ouput memory where all sentences/sequences `x` are stored encoded in an embedding C; this memory contains the seed elements that, together with `q`, lead to an answer `a`.\n",
    "- The generator of the final prediction of the layer, which combines the items in both.\n",
    "\n",
    "Then, these single layers are combined as recurrent layers that perform several hops or steps to obtain the final output.\n",
    "\n",
    "A single layer consists of the following operations, and it is depicted in the figure below:\n",
    "\n",
    "- given `x_i`, we compute in th einput memory: `m_i <- EmbeddingA(x_i)`\n",
    "- we store the vector `m_i` in the input memory\n",
    "- given `q`, we compute: `u <- EmbeddingB(q)`\n",
    "- we store the vector `u` in the input memory\n",
    "- we compute `p_i` as: `p_i = softmax(u^T * m_i)` (inner product)\n",
    "- given `x_i`, we compute in the output memory: `c_i <- EmbeddingC(x_i)`\n",
    "- the output vector `o` is the sum of `c_i` weighted by `p_i`: `o <- sum(p_i*c_i)`\n",
    "- the predicted answer `a` is: `a <- softmax(W(o+u))`\n",
    "    - `W` is a weight matrix of size `V x d`\n",
    "    - `V`: number of symbols/tokens in the vocabulary/dictionary\n",
    "    - `d`: size of the compact sequence vectors in the embedding\n",
    "    \n",
    "Note that everything is differentiable. That makes possible to use backpropagation, i.e., we can train the layers to optimize the weights of `W`.\n",
    "\n",
    "![End-To-End memory Networks](../pics/end_to_end_memory_networks.png)\n",
    "\n",
    "The recurrent aspect is achieved by stacking one layer after the other. All layers get all sentences of the story `x` and the same question `q`, but the output from the previous layer is summed to the question in the next layer.\n",
    "\n",
    "### Model & Training Details\n",
    "\n",
    "- `K = 3` hops/steps or layers were used.\n",
    "- Tyically, the answr is a single words, but sometimes several.\n",
    "- Some sentences in the story are irrelevant for the answer; the relevant ones are called **support** and the model detects them.\n",
    "- The sentences are represented initially as bags of words; each sentence `x_i` in a story is transformed in `m_i` by summing all the vector representations of all words after applying the embedding.\n",
    "- In order to preserve the order aspect of the words in a sentence and the temporal aspect of the sentences in a story, some weights/encoding are applied element-wise to the summations.\n",
    "- They found that adding random noise to the temporal encoding improved the performance.\n",
    "- Learning rate: `lr = 0.01`, annealing to `lr/2` every 25 epochs until 100 epochs were reached.\n",
    "- The `null` symbol was used for padding.\n",
    "\n",
    "### Simplified Example in the Notebook\n",
    "\n",
    "The example in this notebook uses a very similar approach, but the dataset consists of Story-Question-Answer items that have only `Yes/No` answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808f6dd2-2957-44a5-9fc9-341d72a556d0",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f50a5aad-3029-41d2-b2ae-59696414d35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ceefc3ba-e308-4f40-afd9-08e6ea51d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch out: the TXT is a binary, thus we need to use pickle to read it\n",
    "with open(\"../data/train_qa.txt\", \"rb\") as fp:   # Unpickling\n",
    "    train_data =  pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ef6ab893-7f95-49fd-a3c6-cefc61963760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch out: the TXT is a binary, thus we need to use pickle to read it\n",
    "with open(\"../data/test_qa.txt\", \"rb\") as fp:   # Unpickling\n",
    "    test_data =  pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6563fdca-28db-4135-8700-f3fcc23bab2e",
   "metadata": {},
   "source": [
    "### 2.1 Understand the Structure of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "946236c4-53d8-4ffe-8e5b-7c9ac5df5b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0d7483a2-13ca-4145-be3d-c21a1241a4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10,000 items for the training data\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4e94dfc2-5b63-4efb-b197-630754cd3a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Mary',\n",
       "  'moved',\n",
       "  'to',\n",
       "  'the',\n",
       "  'bathroom',\n",
       "  '.',\n",
       "  'Sandra',\n",
       "  'journeyed',\n",
       "  'to',\n",
       "  'the',\n",
       "  'bedroom',\n",
       "  '.'],\n",
       " ['Is', 'Sandra', 'in', 'the', 'hallway', '?'],\n",
       " 'no')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick and display one item: Story, Question, Answer.\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b6b662b3-b799-4577-9d90-a11abb07de1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mary moved to the bathroom . Sandra journeyed to the bedroom .'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the Story\n",
    "' '.join(train_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ef4c4b53-5d47-4eb1-8220-04836645b02e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is Sandra in the hallway ?'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the Question\n",
    "' '.join(train_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fd2f6118-bb06-4227-9065-7db907f9b0b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the Answer: IT IS ALWAYS YES/NO!\n",
    "train_data[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742a8eee-5ff1-4196-a700-25d7a379ba79",
   "metadata": {},
   "source": [
    "### 2.2 Set Up the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "99e469ca-cef7-4b40-84ed-3ab91c12e961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set that holds the vocab words\n",
    "# Set: unordered collection of unique items\n",
    "vocab = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3743fed2-0eef-45e5-a0b7-3674b6c2823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate both splits\n",
    "all_data = test_data + train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c2071906-dd09-49dd-9849-961d574e4875",
   "metadata": {},
   "outputs": [],
   "source": [
    "for story, question , answer in all_data:\n",
    "    # In case you don't know what a union of sets is:\n",
    "    # https://www.programiz.com/python-programming/methods/set/union\n",
    "    # Basically, a union is done: all distinct items are taken to create a new set.\n",
    "    vocab = vocab.union(set(story))\n",
    "    vocab = vocab.union(set(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c4a46694-ada5-4f11-b4a1-b4318a6ad266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the Answer is always Yes/No, we add these words manually\n",
    "vocab.add('no')\n",
    "vocab.add('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "61a17b16-2af5-4b0f-9489-49f58a7a9caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " '?',\n",
       " 'Daniel',\n",
       " 'Is',\n",
       " 'John',\n",
       " 'Mary',\n",
       " 'Sandra',\n",
       " 'apple',\n",
       " 'back',\n",
       " 'bathroom',\n",
       " 'bedroom',\n",
       " 'discarded',\n",
       " 'down',\n",
       " 'dropped',\n",
       " 'football',\n",
       " 'garden',\n",
       " 'got',\n",
       " 'grabbed',\n",
       " 'hallway',\n",
       " 'in',\n",
       " 'journeyed',\n",
       " 'kitchen',\n",
       " 'left',\n",
       " 'milk',\n",
       " 'moved',\n",
       " 'no',\n",
       " 'office',\n",
       " 'picked',\n",
       " 'put',\n",
       " 'the',\n",
       " 'there',\n",
       " 'to',\n",
       " 'took',\n",
       " 'travelled',\n",
       " 'up',\n",
       " 'went',\n",
       " 'yes'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display all possible vocabulary words.\n",
    "# Note that they are not that much.\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7bd1af57-a1f8-40b0-91ff-889ff0276666",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = len(vocab) + 1 # we add an extra space to hold a 0 for Keras's pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "14d91f75-c26e-4331-81fc-80e556db5268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1f5d57e6-89fe-4a4a-a968-3796c4e94c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How long is the longest Story?\n",
    "max_story_len = max([len(data[0]) for data in all_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5850ceb-d43f-44ad-a045-0c62a114d611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_story_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fc943d7e-0be4-4bf4-af3a-ac82a0b0d219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How long is the longest Question?\n",
    "max_question_len = max([len(data[1]) for data in all_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "af5965fa-755c-4155-ad84-7a0f26dc7c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_question_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21810d9-0de1-4a0a-b5df-323b152bf774",
   "metadata": {},
   "source": [
    "### 2.3 Vectorize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50917b37-9df6-48c2-b086-ce528edb9159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bcb55095-bbaa-46c3-96a7-da0693d2b36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integer encoded sequences of words\n",
    "# We don't need to use any filters with our dataset\n",
    "tokenizer = Tokenizer(filters=[])\n",
    "tokenizer.fit_on_texts(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c374ba0e-2df6-4413-8b15-0ad9a2b284c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple': 1,\n",
       " 'no': 2,\n",
       " '?': 3,\n",
       " 'yes': 4,\n",
       " 'bathroom': 5,\n",
       " 'grabbed': 6,\n",
       " 'down': 7,\n",
       " '.': 8,\n",
       " 'hallway': 9,\n",
       " 'bedroom': 10,\n",
       " 'kitchen': 11,\n",
       " 'dropped': 12,\n",
       " 'is': 13,\n",
       " 'mary': 14,\n",
       " 'picked': 15,\n",
       " 'daniel': 16,\n",
       " 'garden': 17,\n",
       " 'left': 18,\n",
       " 'there': 19,\n",
       " 'moved': 20,\n",
       " 'to': 21,\n",
       " 'office': 22,\n",
       " 'sandra': 23,\n",
       " 'milk': 24,\n",
       " 'discarded': 25,\n",
       " 'journeyed': 26,\n",
       " 'in': 27,\n",
       " 'back': 28,\n",
       " 'up': 29,\n",
       " 'john': 30,\n",
       " 'got': 31,\n",
       " 'took': 32,\n",
       " 'football': 33,\n",
       " 'went': 34,\n",
       " 'travelled': 35,\n",
       " 'the': 36,\n",
       " 'put': 37}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary/Map of word->index\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "25af2961-3080-445b-be7d-8ac723ce417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack Story/Question/Answer to lists\n",
    "train_story_text = []\n",
    "train_question_text = []\n",
    "train_answers = []\n",
    "for story,question,answer in train_data:\n",
    "    train_story_text.append(story)\n",
    "    train_question_text.append(question)\n",
    "    train_answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1667a932-5637-463f-9906-ea1e04e17f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text/string sequences to integer sequences\n",
    "train_story_seq = tokenizer.texts_to_sequences(train_story_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ea1e0242-7908-49ea-b391-36e207072b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mary',\n",
       " 'moved',\n",
       " 'to',\n",
       " 'the',\n",
       " 'bathroom',\n",
       " '.',\n",
       " 'Sandra',\n",
       " 'journeyed',\n",
       " 'to',\n",
       " 'the',\n",
       " 'bedroom',\n",
       " '.']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_story_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f04b1ddd-dfc0-4611-a337-e5502206a554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 20, 21, 36, 5, 8, 23, 26, 21, 36, 10, 8]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_story_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "670469e5-c78e-4ea4-92f8-ae4b9c55c230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_story_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9870dd52-39df-48ea-aa6d-eac3277995e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_story_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd4a5cc-7e7d-494b-8e1d-e3dbdc7d6740",
   "metadata": {},
   "source": [
    "### 2.4 Functionalize the Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f0f902-b849-457d-921c-277b33bfb94b",
   "metadata": {},
   "source": [
    "In the following, we create a function that creates valid input vectors of a dataset. Basically, S-Q-A texts are converted into integer/word-index matrices. Note that the answers are allowed to have the same dimension as the size of the vocabulary (`+1`, because of Keras convention). I understand that the logic is the following: originally, an answer of a single word was allowed, no matter which word in the vocabulary; thus, the answer is a dummy/one-hot endoded vectors. In the present simplified implementation, that same architecture/design is used, but through the dataset and its training, only the words `yes`/`no` appear in the answers. Thus, I could easily modify this example to get any single word as answer (one word from the vocabulary) -- only, I would require a valid dataset with any single word as answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "467a5fc2-acbf-4f0f-aae1-5356e7c290e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_stories(data,\n",
    "                      word_index=tokenizer.word_index,\n",
    "                      max_story_len=max_story_len,\n",
    "                      max_question_len=max_question_len):\n",
    "    '''\n",
    "    INPUT: \n",
    "    \n",
    "    data: consisting of Stories, Queries, and Answers\n",
    "    word_index: word index dictionary from tokenizer\n",
    "    max_story_len: the length of the longest story (used for pad_sequences function)\n",
    "    max_question_len: length of the longest question (used for pad_sequences function)\n",
    "\n",
    "    OUTPUT:\n",
    "    \n",
    "    Vectorizes the stories, questions, and answers into padded sequences.\n",
    "    We first loop for every story, query, and answer in the data.\n",
    "    Then we convert the raw words to an word index value.\n",
    "    Then we append each set to their appropriate output list.\n",
    "    Then, once we have converted the words to numbers,\n",
    "    we pad the sequences so they are all of equal length.\n",
    "    \n",
    "    Returns this in the form of a tuple (X, Xq, Y) (padded based on max lengths)\n",
    "    '''\n",
    "    \n",
    "    # X = STORIES\n",
    "    X = []\n",
    "    # Xq = QUERY/QUESTION\n",
    "    Xq = []\n",
    "    # Y = CORRECT ANSWER (yes/no)\n",
    "    Y = []\n",
    "    \n",
    "    # For each Story-Question-Answer\n",
    "    for story, query, answer in data:\n",
    "        \n",
    "        # Grab the word index for every word in story\n",
    "        x = [word_index[word.lower()] for word in story]\n",
    "        # Grab the word index for every word in query\n",
    "        xq = [word_index[word.lower()] for word in query]\n",
    "        \n",
    "        # Grab the Answers (either Yes/No so we don't need to use list comprehension here)\n",
    "        # Index 0 is reserved so we're going to use + 1\n",
    "        # We create an array of size the number of vocabulary items\n",
    "        y = np.zeros(len(word_index) + 1)\n",
    "        \n",
    "        # Now that y is all zeros and we know it's just Yes/No,\n",
    "        # we can use numpy logic to create this assignment\n",
    "        # Everything is 0 except the answer word (yes/no)\n",
    "        # Why? In understand that originally the answer is supposed to be one word\n",
    "        # not just yes/no. Thus, here the architecture/structure is maintained,\n",
    "        # but the model is trained only with yes/no.\n",
    "        y[word_index[answer]] = 1\n",
    "        \n",
    "        # Append each set of story, query, and answer to their respective holding lists\n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        Y.append(y)\n",
    "        \n",
    "    # Finally, pad the sequences based on their max length so the RNN can be trained on uniformly long sequences.\n",
    "        \n",
    "    # RETURN TUPLE FOR UNPACKING\n",
    "    return (pad_sequences(X, maxlen=max_story_len),\n",
    "            pad_sequences(Xq, maxlen=max_question_len),\n",
    "            np.array(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "11d889dc-fc5a-43de-9b2d-5e54b99d5dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train, queries_train, answers_train = vectorize_stories(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5a394f34-f1b5-4828-8908-bf802a030a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test, queries_test, answers_test = vectorize_stories(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f82884b3-a55c-4de9-a21f-fe084bb45fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 156)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of S-Q-A items x max_story_len (maximum number of words in a story)\n",
    "inputs_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ce421b16-a9c1-4622-88b6-ba01aedccd16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ..., 36, 10,  8],\n",
       "       [ 0,  0,  0, ..., 36, 17,  8],\n",
       "       [ 0,  0,  0, ..., 36, 17,  8],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ..., 36,  1,  8],\n",
       "       [ 0,  0,  0, ..., 36, 17,  8],\n",
       "       [ 0,  0,  0, ...,  1, 19,  8]], dtype=int32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "de2b6008-2d3a-462f-967c-bebda0bbc766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 6)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of S-Q-A items x max_question_len (maximum number of words in a story)\n",
    "queries_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "056aba2f-88d4-48b7-9b45-ba95406db76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13, 30, 27, 36, 11,  3],\n",
       "       [13, 30, 27, 36, 11,  3],\n",
       "       [13, 30, 27, 36, 17,  3],\n",
       "       ...,\n",
       "       [13, 14, 27, 36, 10,  3],\n",
       "       [13, 23, 27, 36, 17,  3],\n",
       "       [13, 14, 27, 36, 17,  3]], dtype=int32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f3e174d7-4afd-4a47-8a64-86b406718e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 38)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of S-Q-A items x (vocabulary length + 1)\n",
    "answers_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f8d9e7a9-33ee-4804-8c2e-ec6780853fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2bbe7a7a-1e5c-4420-8ec5-b71915aed778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0., 503.,   0., 497.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All answers must be yes (index 4) or no (index 2)\n",
    "# Thus, the sum must be concentrated in those two elements/indices\n",
    "sum(answers_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1796402-ccfb-46e5-8558-6e0a83cfc67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bf434dd9-74bb-4d5a-b78c-6be0b3f39e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['no']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fefd1ba-a170-40db-840a-6dfbe03314da",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6650871f-a954-4885-b133-a90d8d017e63",
   "metadata": {},
   "source": [
    "In this section, the DL model that predicts answers given a story and a question is built. The model is strongly based on the one suggested in the paper by Sukhbaatar et al. referenced above. Read the paper or the notes to understand the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b19f8dbe-4f3d-4f2f-ba01-69f76d65c040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout\n",
    "from keras.layers import add, dot, concatenate\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "38e311ac-dd46-43f8-b3e0-b226ca5b476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input() is used to instantiate Keras tensors\n",
    "input_sequence = Input((max_story_len,))\n",
    "question = Input((max_question_len,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce99329-b7b6-4ea6-8d17-5adf053ccdc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
